{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 既成モデルを利用して顔認証を最速で実装する\n",
    "- `MTCNN`, `keras_vggface`を利用した顔認証の実装例です。\n",
    "- 2019年時点では本当に早かったのですが今(Mar.11.2020)動かすと`keras`のAPI変更などのため微妙です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 画像ロード\n",
    "- 何でもよいので画像を読み込みます。\n",
    "  - この例では`True`を渡すことで、Webカメラなりのソースを`VideoCapture`するようにしました。\n",
    "  - `False`を渡すといつもの人を読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本セットのimport\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_image(useCam):\n",
    "    if useCam:\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        _,origin = cap.read()\n",
    "        cap.release()\n",
    "        origin = cv2.cvtColor(origin, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "        origin = plt.imread('https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png')\n",
    "    return origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 顔の検出\n",
    "- 極論やらなくてもよいのですが、顔の部分だけ抜粋してあげた方が親切だよねということで、顔の検出を行います。\n",
    "- 一枚の画像に複数の顔があり、それぞれ認証したいです、というような場合はここで工夫してあげる必要があります。\n",
    "- ※2020年現在`MTCNN`が`keras`の最新APIにキャッチアップしていません。\n",
    "  - https://github.com/ipazc/mtcnn/issues/83\n",
    "  - 上記issueのとおり`factory.py`を編集するとエラー自体は回避できます。\n",
    "  - 抽出もうまくいかないようですが、メンテナンスは継続しているようなので、そのうち対応してくれるかもしれません。\n",
    "- 今回は、顔抽出に失敗した場合は画像をそのまま特徴抽出に流すものとして進めます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mtcnn\n",
    "import PIL.Image as Image\n",
    "detector = mtcnn.MTCNN()\n",
    "\n",
    "# 顔抽出メソッド\n",
    "def extract_face(source):\n",
    "    face = source\n",
    "    faces = detector.detect_faces(source)\n",
    "    print(f'抽出した顔の数:{len(faces)}')\n",
    "    if len(faces) > 0:\n",
    "        # 開始ピクセルのX,Yおよび判定結果の横幅、縦幅を取得\n",
    "        x1, y1, w, h, = faces[0]['box']\n",
    "        # 終端ピクセルを決定\n",
    "        x2, y2 = x1 + w, y1 + h\n",
    "        face = source[y1:y2, x1:x2]\n",
    "        image = Image.fromarray(face)\n",
    "    else:\n",
    "        image = Image.fromarray((face * 255).astype(np.uint8))\n",
    "    resized = image.resize((224, 224), Image.ANTIALIAS)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルに渡すための加工メソッド\n",
    "import numpy\n",
    "\n",
    "def to_processable(face):\n",
    "    pixels = numpy.asarray(face)\n",
    "    pixels = pixels.astype('float32')\n",
    "    return numpy.expand_dims(pixels, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.特徴の抽出\n",
    "- `vggface`が提供するモデルに画像を渡して、特徴を取得します。\n",
    "  - 試した限り、英語圏の人物にはかなり優秀ですが、日本人についてはやや信頼性が落ちるようです。\n",
    "    - 実際問題それはやむを得ないところなので、精度を出したい場合は日本人の顔データで転移学習なり\n",
    "- ※`keras_vggface`においても`MTCNN`同様、`keras`から`tensorflow.keras`への移行が必要となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽出した顔の数:0\n"
     ]
    }
   ],
   "source": [
    "import keras_vggface as vgg\n",
    "import keras_vggface.utils as vggutil\n",
    "\n",
    "# ここまで作ったメソッドを呼び出し、顔データを取得\n",
    "source = get_image(False)\n",
    "face = extract_face(source)\n",
    "samples = to_processable(face)\n",
    "\n",
    "# モデルに顔データを渡す\n",
    "samples = vggutil.preprocess_input(samples, version=2)\n",
    "model = vgg.VGGFace(model='resnet50', include_top=False, input_shape=(224,224,3), pooling='avg')\n",
    "features = model.predict(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 特徴の比較\n",
    "- 特徴(ベクター)間の距離を比較することで、同一人物であるか別人であるかの判定を行います。\n",
    "- 今回のセットアップでは`100`程度を閾値としてあげると丁度よい感じになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽出した顔の数:0\n"
     ]
    }
   ],
   "source": [
    "# 二枚目の画像を用意し、特徴を抽出\n",
    "face2 = get_image(False)\n",
    "face2 = extract_face(face2)\n",
    "samples2 = to_processable(face2)\n",
    "feature2 = model.predict(samples2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "同一人物です (特徴差スコア：19.01454734802246)\n"
     ]
    }
   ],
   "source": [
    "# 比較\n",
    "import scipy.spatial\n",
    "dist = scipy.spatial.distance.euclidean(features, feature2)\n",
    "if (dist < 100):\n",
    "    print(f'同一人物です (特徴差スコア：{dist})')\n",
    "else:\n",
    "    print(f'違う人物です (特徴差スコア：{dist})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
